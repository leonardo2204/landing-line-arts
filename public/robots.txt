# Allow all crawlers
User-agent: *
Allow: /

# Disallow sensitive directories
Disallow: /node_modules/
Disallow: /.bolt/
Disallow: /dist/
Disallow: /supabase/

# Sitemap location (uncomment and update when you have a sitemap)
# Sitemap: https://your-domain.com/sitemap.xml

# Allow all major search engines
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

User-agent: YandexBot
Allow: /

User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

# Block AI training bots (optional - you can remove these if you want AI training)
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

# Block common spam/scraper bots
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: SemrushBot
Disallow: /

# Crawl delay for respectful crawling
Crawl-delay: 1

# Additional directives for better SEO
# Allow access to CSS and JS files for proper rendering
Allow: /assets/
Allow: /*.css
Allow: /*.js
Allow: /*.png
Allow: /*.jpg
Allow: /*.jpeg
Allow: /*.gif
Allow: /*.svg
Allow: /*.webp

# Block access to sensitive files (if any exist)
Disallow: /admin/
Disallow: /.env
Disallow: /config/
Disallow: /logs/
Disallow: /tmp/
Disallow: /backup/